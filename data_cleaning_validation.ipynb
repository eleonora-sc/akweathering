{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mineral Deposits and Sediment Samples - Exploratory Data Analysis\n",
    "This notebook performs exploratory data analysis including:<br>\n",
    "1. Descriptive statistics for each element (mean, median, standard deviation, count) and distribution.<br>\n",
    "2. Box plots broken up by element category (major, minor, trace).<br>\n",
    "3. Correlation Matrix.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data\n",
    "For testing purposes we are just loading in the first 10000 entries of the AKSediment csv, called 'AKSediment_truncated.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eleonora\\AppData\\Local\\Temp\\ipykernel_5780\\2502602361.py:4: DtypeWarning: Columns (9,18,19,35,36,39,45,71,72,104,105,128,129,137,138,143,144,149,150,161,162,164,165,167,168,173,174,191,192,197,198,206,207,212,213,215,216,218,219,224,225,227,228,230,231,245,246,248,249,257,258,263,264,266,267,272,273,278,279,290,291) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sediment_df = pd.read_csv(sediment_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in sediment_df: 10000\n",
      "Number of columns in sediment_df: 300\n",
      "Number of rows in mindeposit_df: 7720\n",
      "Number of columns in mindeposit_df: 30\n"
     ]
    }
   ],
   "source": [
    "sediment_csv = 'data/AKSediment_truncated.csv'\n",
    "mindeposit_csv = 'data/original/AKMinDeposits.csv'\n",
    "\n",
    "sediment_df = pd.read_csv(sediment_csv)\n",
    "mindeposit_df = pd.read_csv(mindeposit_csv)\n",
    "\n",
    "# Print out how many rows/columns are in the dataframes and also the first few rows of the dataframes,uncomment as needed\n",
    "# print(sediment_df.head(10))\n",
    "print(\"Number of rows in sediment_df:\", sediment_df.shape[0])\n",
    "print(\"Number of columns in sediment_df:\", sediment_df.shape[1])\n",
    "# print(mindeposit_df.head(10))\n",
    "print(\"Number of rows in mindeposit_df:\", mindeposit_df.shape[0])\n",
    "print(\"Number of columns in mindeposit_df:\", mindeposit_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Validation for Mineral Deposit Data\n",
    "First we will filter by location and then we will filter by mineral. <br><br>\n",
    "\n",
    "We will start by looking just at the mineral deposits in the Brooks Range area. The lat and long boundaries used are rough testing boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define latitude and longitude boundaries\n",
    "min_lat, max_lat = 65, 69\n",
    "min_lon, max_lon = -166, -141\n",
    "\n",
    "# Filter the DataFrame for rows within the specified range\n",
    "mindeposit_area_filtered_df = mindeposit_df[(mindeposit_df['latitude'] >= min_lat) & \n",
    "                        (mindeposit_df['latitude'] <= max_lat) & \n",
    "                        (mindeposit_df['longitude_for_GIS'] >= min_lon) & \n",
    "                        (mindeposit_df['longitude_for_GIS'] <= max_lon)]\n",
    "\n",
    "# # Output the result for testing, uncomment as needed\n",
    "# mindeposit_area_filtered_df.to_csv('mindeposit_area_filtered.csv', index=False)\n",
    "# print(\"Number of rows in mindeposit_area_filtered_df:\", mindeposit_area_filtered_df.shape[0])\n",
    "# print(\"Number of columns in mindeposit_area_filtered_df:\", mindeposit_area_filtered_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by looking just at copper and zinc deposits in the mineral deposits data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindeposit_mineral_filtered_df = mindeposit_area_filtered_df[\n",
    "    (mindeposit_area_filtered_df['commodities_main'].str.contains('Cu', na=False) | \n",
    "     mindeposit_area_filtered_df['commodities_other'].str.contains('Cu', na=False)) |\n",
    "    (mindeposit_area_filtered_df['commodities_main'].str.contains('Zn', na=False) | \n",
    "     mindeposit_area_filtered_df['commodities_other'].str.contains('Zn', na=False))\n",
    "]\n",
    "\n",
    "# # Output the result for testing, uncomment as needed\n",
    "# mindeposit_mineral_filtered_df.to_csv('mindeposit_mineral_filtered.csv', index=False)\n",
    "# print(\"Number of rows in mindeposit_mineral_filtered_df:\", mindeposit_mineral_filtered_df.shape[0])\n",
    "# print(\"Number of columns in mindeposit_mineral_filtered_df:\", mindeposit_mineral_filtered_df.shape[1])\n",
    "\n",
    "# Save the cleaned data to the /cleaned dir\n",
    "mindeposit_mineral_filtered_df.to_csv('AKMinDeposits_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Validation for Sediment Data\n",
    "We will start by looking just at the mineral deposits in the Brooks Range area. The lat and long boundaries used are rough testing boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define latitude and longitude boundaries\n",
    "min_lat, max_lat = 65, 69\n",
    "min_lon, max_lon = -166, -141\n",
    "\n",
    "# Filter the DataFrame for rows within the specified range\n",
    "sediment_area_filtered_df = sediment_df[(sediment_df['LATITUDE'] >= min_lat) & \n",
    "                        (sediment_df['LATITUDE'] <= max_lat) & \n",
    "                        (sediment_df['LONGITUDE'] >= min_lon) & \n",
    "                        (sediment_df['LONGITUDE'] <= max_lon)]\n",
    "\n",
    "# # Output the result for testing, uncomment as needed\n",
    "# sediment_area_filtered_df.to_csv('sediment_area_filtered.csv', index=False)\n",
    "# print(\"Number of rows in sediment_area_filtered_df:\", sediment_area_filtered_df.shape[0])\n",
    "# print(\"Number of columns in sediment_area_filtered_df:\", sediment_area_filtered_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will grab only the rows we are interested in: LATITUDE, LONGITUDE, and chemical elements in ppm or pct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to explicitly keep\n",
    "columns_to_keep = ['LATITUDE', 'LONGITUDE']\n",
    "\n",
    "# Filter for columns that end with '_ppm' or '_pct'\n",
    "filtered_columns = sediment_area_filtered_df.filter(regex='(_ppm|_pct)$').columns\n",
    "\n",
    "sediment_column_filtered_df = sediment_area_filtered_df[columns_to_keep + list(filtered_columns)] \n",
    "\n",
    "# # Output the result for testing, uncomment as needed\n",
    "# sediment_column_filtered_df.to_csv('sediment_column_filtered.csv', index=False)\n",
    "# print(\"Number of rows in sediment_column_filtered_df:\", sediment_column_filtered_df.shape[0])\n",
    "# print(\"Number of columns in sediment_column_filtered_df:\", sediment_column_filtered_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert the values stored as percentages to ppm. Before doing that, we verify that all columns containing ppm or pct values are of type float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns ending in _ppm or _pct are of type float.\n"
     ]
    }
   ],
   "source": [
    "# pct to ppm scaling factor\n",
    "PCT_T0_PPM = 10000 # TODO CHECK THIS WITH JORDAN\n",
    "\n",
    "# Filter columns ending with '_ppm' or '_pct'\n",
    "filtered_columns = sediment_column_filtered_df.filter(regex='(_ppm|_pct)$').columns\n",
    "\n",
    "# Check if all these columns are of type float\n",
    "ALL_FLOATS = sediment_column_filtered_df[filtered_columns].map(lambda x: isinstance(x, float)).all().all()\n",
    "\n",
    "# If all these columns are of type float, convert pct columns to ppm\n",
    "if ALL_FLOATS:\n",
    "    print('All columns ending in _ppm or _pct are of type float.')\n",
    "    \n",
    "    # Find all columns ending with '_pct'\n",
    "    pct_columns = sediment_column_filtered_df.filter(regex='_pct$').columns\n",
    "\n",
    "    sediment_pct_to_ppm_df = sediment_column_filtered_df.copy()\n",
    "\n",
    "    # Multiply values in the _pct columns by the scaling factor\n",
    "    sediment_pct_to_ppm_df.loc[:, pct_columns] *= PCT_T0_PPM\n",
    "\n",
    "    # Rename the columns ending in _pct to ending in _ppm\n",
    "    sediment_pct_to_ppm_df.rename(columns=lambda x: x.replace('_pct', '_ppm') if x.endswith('_pct') else x, inplace=True)\n",
    "\n",
    "    # # Output the result for testing, uncomment as needed\n",
    "    # sediment_pct_to_ppm_df.to_csv('sediment_pct_to_ppm.csv', index=False)\n",
    "    # print(\"Number of rows in sediment_pct_to_ppm_df:\", sediment_pct_to_ppm_df.shape[0])\n",
    "    # print(\"Number of columns in sediment_pct_to_ppm_df:\", sediment_pct_to_ppm_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values below -1000 should be replaced with NaN as they are likely errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values below and including -1000 have been successfully removed.\n"
     ]
    }
   ],
   "source": [
    "# Filter columns ending with '_ppm' or '_pct'\n",
    "filtered_columns = sediment_pct_to_ppm_df.filter(regex='_ppm$').columns\n",
    "\n",
    "sediment_adjusted_values_nan_df = sediment_pct_to_ppm_df.copy()\n",
    "\n",
    "# Replace values below and including -1000 with NaN\n",
    "sediment_adjusted_values_nan_df[filtered_columns] = sediment_adjusted_values_nan_df[filtered_columns].mask(sediment_adjusted_values_nan_df[filtered_columns] <= -1000, np.nan)\n",
    "\n",
    "# Unit test using assert\n",
    "for column in filtered_columns:\n",
    "    # Check if there are any values below or equal to -1000\n",
    "    below_threshold = (sediment_adjusted_values_nan_df[column] <= -1000)\n",
    "    # Count of values below or equal to -1000\n",
    "    count_below_threshold = below_threshold.sum()\n",
    "    # Assert that count is 0\n",
    "    assert count_below_threshold == 0, f\"Column '{column}' contains {count_below_threshold} values below or equal to -1000.\"\n",
    "\n",
    "print(\"All values below and including -1000 have been successfully removed.\")\n",
    "\n",
    "# # Output the result for testing, uncomment as needed\n",
    "# sediment_adjusted_values_nan_df.to_csv('sediment_adjusted_values_nan.csv', index=False)\n",
    "# print(\"Number of rows in sediment_adjusted_values_nan_df:\", sediment_adjusted_values_nan_df.shape[0])\n",
    "# print(\"Number of columns in sediment_adjusted_values_nan_df:\", sediment_adjusted_values_nan_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other negative values should be replaced with half their magnitude (half the detection limit). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment_adjusted_values_detection_df = sediment_adjusted_values_nan_df.copy()\n",
    "\n",
    "# For other negative values, replace them with half their maginute (approximately half of the detection limit)\n",
    "sediment_adjusted_values_detection_df[filtered_columns] = sediment_adjusted_values_detection_df[filtered_columns].mask(\n",
    "    sediment_adjusted_values_detection_df[filtered_columns] < 0, abs(sediment_adjusted_values_detection_df[filtered_columns] / 2))\n",
    "\n",
    "# # Output the result for testing, uncomment as needed\n",
    "# sediment_adjusted_values_detection_df.to_csv('sediment_adjusted_values_detection.csv', index=False)\n",
    "# print(\"Number of rows in sediment_adjusted_values_detection_df:\", sediment_adjusted_values_detection_df.shape[0])\n",
    "# print(\"Number of columns in sediment_adjusted_values_detection_df:\", sediment_adjusted_values_detection_df.shape[1])\n",
    "\n",
    "# Save the cleaned data to the /cleaned dir\n",
    "sediment_adjusted_values_detection_df.to_csv('AKSediment_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
